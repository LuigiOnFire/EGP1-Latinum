From a top level perspective, we need to implement

pred = model(X)
loss = loss_fn(pred, y)

and hence get the logits (from which we will derive the loss)

Looking into model

######
GPT
######

------
Constructor
------

In both, GPT is the top level object, which initialized all the other neural network modules and strings them together to make the full pass

In both GPT takes in its construction a config object--this should be familiar from the homework

Does a bunch of asserts to make sure the input configs are valid (i.e. just not None)

Our transformer consits of:
(how do we initialize our transformer? it is an nn.ModuleDict)
token embeddings layer: nn.Embedding(number of tokens, embedding_dim)
position embeddings layer: nn.Embedding(block length, embedding_dim)
dropout layer nn.Dropout(dropout from config) (what's a good dropout value?)
all our blocks, the number of blocks come from block_count in our config
Layernorm(embedding_dim, optional bias from config)

Outside of the transformer there is also lmhead(embedding_dim, token_number, bias=False)

Here's a step that only nano has, described here https://paperswithcode.com/method/weight-tying
self.transformer.wte.weight = self.lm_head.weight


------
Init Weights
------
self.__init_weights is called using self.apply
self.apply on a nn.module takes its every submodule, that's why we have the case setup for each kind of submodule (linear, embedding, and layernorm on mini only)

Layernorm is initalized in its constructor in nano (probably cleaner?) but with evertyhing else in _init_weights in mini

!!!Actually we do want to update layernorm in InitWeights, the nano version was a workaround for an old version of pytorch!!!

Probably something like this:

# TODO: This might not work, check for true and false biases!
elif isinstance(module, nn.LayerNorm):
    torch.nn.init.zeros_(module.bias) if module.bias
    torch.nn.init.ones_(module.weight) 

Linear models are intiialized using normal_(module.weight, mean = 0, std=0.2)
then we check if the module.bias is not None, and if it isn't, we initialize the biases to 0 using torch.nn.init.zeros_(module.bias)

If instead it's an embedding we initailized it using normal in the same way as a lienar model


------
From Pretrained
------

both have a big "from pretrained" function, but that's for a huggingface checkpoint

------
Forward
------

the "forward" function
gets the device from idx (input block) and sets its device to that
gets batch and sequence length from idx's dimensions
checks that that sequence length isn't too big
uses torch.arrange() to make the positoin encoding (mini unsqueezes another dimension)
feeds the tokens to the token embedding layer
feeds the arrange result into the positoin embedding layer
does dropout...
then each block...
then ln_f
then gets logits from lm_head
gets the loss from the cross entropy loss (which has to do some resizing)

Questions:

How do we "check" that the input isn't too big for the model?
Compare the second dimension of input from input.size() and make sure it's less than or equal self.config.block_size

How do we forward with the Module Dict?
for block in self.transformer.h:
    x = block(x)

What do we need to do the dropout?
    x = self.transformer.drop(tok_emb + pos_emb)
    Note that this adds the two vectors and THEN performs dropout

logits is the output of lm_head. Even though it's returned in the training script, it's not used.
For my purpose we can treat logits as pred and do the loss function elswhere
loss is just cross entropy between Y and logits


------
Configure Optimizers
------

next configure_optimizers() function
both versions have different ways of filtering out parameters to have different weight decays. mini is more explicit with more direct control and nano is more compact


------
Generate
------
WILL DO LATER

Generate function is for texting, takes a number of tokens to generate, clips the input idx if necessary, then uses various methods we've learned about to generate the tokens from logits

nano has a get_num_params function that is used by an "estimate millflops" function and just in the constructor of GPT

estimate_mfu is the function that esimates the milliflops, to compare with tha PALM paper it looks like

------
Crop Block size
------

nano has crop_block_size, which takes a block size and, after asserting that the input is smaller than the actual block size, clips the block size config stat and then updates the parameters as necessary

nano's crop block size is only used in the training script

Things other than GPT object

mini implements its own NewGELU, nano just uses GELU (what's the difference?) The implementation seems the same as the documentation's definition--just use pytorche'same

In CausalSelfAttention, nano has a term to support "flash attention" to make things faster but it requires pytorch 2.0 +
This is in both the constructor and the forward function

MLP is it's own function in nano--seems clean let's do that

######
LayerNorm
######

Actually we won't imlement this--it's no longer needed since pytorch now allows us to write bias=false

######
CausalSelfAttention
######

------
Constructor
------

FIRST we check that the number of embeddings is divisible by the head count
assert that the remainder between the embedding dim and the head is 0

We make a linear layer for attention Linear(embed_dim, 3 * embed_dim)
Then a linear layer for projection Linear(embed_dim, embed_dim)

Then an attentiion dropout
nn.Dropout(config.dropout) (you could make this its own param if you want but meh)

and a residual dropout
nn.Dropout(config.dropout)

Now make the following config variables instance variables

number of heads
embedding dim
dropout rate

We can check if oru pytorch has flash (much faster)
self.flash_attn = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
if not self.flash_attn:
    # warn the user, tell them Pytorch 2.0 is better
    self.register_buffer(
        "bias", 
        torch.trill(
            torch.ones(
                config.block_size, config. block_size)).view(
                    1,1,config.block_size, config.block_size
                )
    )    

------
Forward
------

batch_size, sequence_length, embedding_dim = x.size()
b, l, d, if we follow the stanford handout

extract q, k, and v from c_attn by splitting c_attn into groups of size embed_dim along dimension 2
reshape each of the above using view into something of size(b, l, d / head_count)
then transpose dimensions 1 and 2 of each of those

next, if we have flash enabled (defined in constructor), or job is easy
We can just call 
torch.nn.functional.scaled_dot_product_attention(q, k, v,
    set attn_mask to None
    set dropout to self.dropout if self has training defined, otherwise set it to 0,
    set is_causal to true
)

if NOT, then we have to actually implement the self attention equations
first matrix multiply q by k (transposed last and second to last dimensions)
then, divide the result by sqrt(k.size(-1)) (this is analogous to d/h in our equation)
Then, get a submatrix of the bias mask matrix to make matrix that will max out future elements to -inf
get the submatrix using self.bias[:,:,:T,:T]
Then, take the softmax of att along the last dimension
finaly, matrix multiply our result with v (the last step of the equation)

Then take your output (for either of the above branches) and transpose elements 1, 2, apply contiguous (to make it more efficient in memory) and view it back into B, T, C shape

finally, put y through the second of our two linear layers and the apply the second dropout
now we can return y!

######
MLP (Multi-level perceptron)
######

The MLP is initialized from config, and contains:
A linear layer(embedding_count, 4*embedding count, bias=config.bias)
A gelu layer which is just nn.GELU()
Another identical linear layer layer(embedding_count, 4*embedding count, bias=config.bias)
a dropout layer which is just Dropout(config.dropout)

Forward is simple, it just forwards each of the above

x = self.module_name(x)

######
Block
######

The block has:
a first layernorm(embed count, config.bias)
Then we do causal self attention (using config)
Then an identical layernorm(embed count, config.bias)
Then put in MLP(config)
For forwarding, we need to add and norm, in other words
> Do the first layer norm
> Add x and the attention layer of X
> take the second layer norm of X
> add x with MLP of x


